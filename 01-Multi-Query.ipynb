{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+JQp0zACX3OndJ9DBn+ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/Retriever/blob/main/01-Multi-Query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNzSQ3VZffUe"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=sk-\" >> .env\n",
        "!source /content/.env\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "# Access the API key using the variable name defined in the .env file\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "IktJaHr9fwj_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "DsRwJ2Bpgt7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "# Loading a single website\n",
        "loader = WebBaseLoader(\"http://www.paulgraham.com/superlinear.html\")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "# Split it up\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=0)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed and store your docs/vectors\n",
        "embedding = OpenAIEmbeddings()\n",
        "vectordb = Chroma.from_documents(documents=chunks, embedding=embedding)"
      ],
      "metadata": {
        "id": "XyQTd4W1f_67"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Set logging for the queries\n",
        "import logging\n",
        "\n",
        "# Set up logging to see your queries\n",
        "logging.basicConfig()\n",
        "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
      ],
      "metadata": {
        "id": "TfTGFif5g8q_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your original question\n",
        "question = \"What are the fundamental use cases of superlinear returns?\"\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBH4wfVahYsD",
        "outputId": "fcbf6adb-aaba-47bf-cb06-dbae01c57748"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_from_llm"
      ],
      "metadata": {
        "id": "D6a00eA7heWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 위 return 값을 parshing 하여  'template'  filed 값을 extract하는 python code 작성"
      ],
      "metadata": {
        "id": "uu9bvUf9iqTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Sample JSON string\n",
        "json_str = \"\"\"\n",
        "{\n",
        "    \"MultiQueryRetriever\": {\n",
        "        \"retriever\": {\n",
        "            \"VectorStoreRetriever\": {\n",
        "                \"tags\": [\"Chroma\", \"OpenAIEmbeddings\"],\n",
        "                \"vectorstore\": \"<langchain_community.vectorstores.chroma.Chroma object at 0x780dc7c39e40>\"\n",
        "            }\n",
        "        },\n",
        "        \"llm_chain\": {\n",
        "            \"LLMChain\": {\n",
        "                \"prompt\": {\n",
        "                    \"PromptTemplate\": {\n",
        "                        \"input_variables\": [\"question\"],\n",
        "                        \"template\": \"You are an AI language model assistant. Your task is \\\\n    to generate 3 different versions of the given user \\\\n    question to retrieve relevant documents from a vector  database. \\\\n    By generating multiple perspectives on the user question, \\\\n    your goal is to help the user overcome some of the limitations \\\\n    of distance-based similarity search. Provide these alternative \\\\n    questions separated by newlines. Original question: {question}\"\n",
        "                    }\n",
        "                },\n",
        "                \"llm\": {\n",
        "                    \"ChatOpenAI\": {\n",
        "                        \"client\": \"<openai.resources.chat.completions.Completions object at 0x780dbae783d0>\",\n",
        "                        \"async_client\": \"<openai.resources.chat.completions.AsyncCompletions object at 0x780dbae79cf0>\",\n",
        "                        \"temperature\": 0.0,\n",
        "                        \"openai_api_key\": \"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\",\n",
        "                        \"openai_proxy\": \"\"\n",
        "                    }\n",
        "                },\n",
        "                \"output_parser\": {\n",
        "                    \"LineListOutputParser\": {\n",
        "                        \"pydantic_object\": \"<class 'langchain.retrievers.multi_query.LineList'>\"\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Parse JSON and extract 'template' field value\n",
        "data = json.loads(json_str)\n",
        "template_value = data[\"MultiQueryRetriever\"][\"llm_chain\"][\"LLMChain\"][\"prompt\"][\"PromptTemplate\"][\"template\"]\n",
        "template_value\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "NjD225rkimLx",
        "outputId": "6e0a502e-cd23-42c0-8d48-697e3bc71927"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are an AI language model assistant. Your task is \\n    to generate 3 different versions of the given user \\n    question to retrieve relevant documents from a vector  database. \\n    By generating multiple perspectives on the user question, \\n    your goal is to help the user overcome some of the limitations \\n    of distance-based similarity search. Provide these alternative \\n    questions separated by newlines. Original question: {question}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"You are an AI language model assistant.\n",
        "\n",
        "Your task is to generate 3 different versions of the given user question to retrieve relevant documents from a vector database.\n",
        "\n",
        "By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations  of distance-based similarity search.\n",
        "\n",
        "Provide these alternative questions separated by newlines.\n",
        "\n",
        "Original question: {question}\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"question\"]\n",
        ")\n",
        "\n",
        "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "    retriever=vectordb.as_retriever(), llm=llm, prompt=PROMPT\n",
        ")"
      ],
      "metadata": {
        "id": "vE1EJuD8jB1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}